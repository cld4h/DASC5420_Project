---
title: "DASC5420 Project: Predicting Stock Market Volatility with Time Series
Models"
author: "Waqar Ul Mulk and Xu Hao"
output: beamer_presentation
---

# First, load the library and download the data

```{r}
library(quantmod) # to download stock data in R
library(dplyr) # dplyr
library(tsibble) # for difference function
# Download data from yahoo Finance!
START.DATE = '2016-04-01' # starting date of stock
END.DATE = '2024-03-31' # ending date of stock
# Download the selected stocks from Yahoo finance 
# using `quantmod` package
getSymbols("AAPL", src = "yahoo", 
	   from = START.DATE, to = END.DATE)
```

# Prepare the data

We are only selecting the adjusted close price `AdjColsed`
and stock volume `Volume` as our predictor.

```{r}
# The `zoo::fortify.zoo` function is used to convert objects of class "zoo" to data frames. The "zoo" package in R provides a flexible and powerful framework for working with irregular time series data.
# Create date variable
apple_stock <- zoo::fortify.zoo(AAPL)
# Rename date variable
apple_stock <- apple_stock %>% rename(c("Date" = "Index", "AdjClosed" = "AAPL.Adjusted", "Volume" = "AAPL.Volume"))
apple <- subset(apple_stock, select = c("Date", "AdjClosed", "Volume"))
knitr::kable(head(apple))
```

# Plot the Price vs Date

Plot the column `apple$AdjClosed` 

```{r fig.width=7, fig.height=3.5}
plot(apple$AdjClosed, type="l", xlab="Date index", 
     ylab="Adjusted close price of Apple", 
     main="Price vs Date")
```
```{r eval=FALSE, echo=FALSE}
pdf("pic/Price_vs_Date.pdf",height=3.5,width=7)
plot(apple$AdjClosed, type="l", xlab="Date index", ylab="Adjusted close price of Apple", main="Price vs Date")
dev.off()
```


# Check autocorrelation

```{r fig.width=7, fig.height=3.5}
acf(apple$AdjClosed, main=expression(paste("ACF of ", 
    Y[t], " (Adjusted Close Price of Apple(AAPL))", sep="")))
```

```{r eval=FALSE, echo=FALSE}
pdf("pic/ACF_AdjClosed.pdf",height=3.5,width=7)
acf(apple$AdjClosed, main=expression(paste("ACF of ", Y[t], " (Adjusted Close Price of Apple(AAPL))", sep="")))
dev.off()
```

We can see that it is highly correlated.

# Calculate the first difference

Here, $Y^{'}_t = Y_t - Y_{t-1}$

```{r fig.width=7, fig.height=3.5}
acf(difference(apple$AdjClosed)[-1], 
    main=expression(paste("ACF of ", 
    Y[t]^"'", sep="")))
```
```{r eval=FALSE, echo=FALSE}
pdf("pic/ACF_dAdjClosed.pdf",height=3.5,width=7)
# -1 here to remove the first NA in the difference vector
acf(difference(apple$AdjClosed)[-1], main=expression(paste("ACF of ", Y[t]^"'", sep="")))
dev.off()
```
There is little autocorrelation after we take the first difference

# Google trends keywords

We get the worldwide interest of 5 apple product over time from google trends using the following python API.

```{py eval=FALSE}
#!pip install pytrends
from pytrends.request import TrendReq
pytrends = TrendReq(hl='en-US', tz=240, timeout=(10,25))
# tz, timezone should be EDT

kw_list = ['Apple Watch', 'MacBook', 'AirPods', 
           'iPad', 'iPhone']
# half-year interval gives the daily trends
tf_list = ['2016-04-01 2016-09-30' ,'2016-10-01 2017-03-31' ,'2017-04-01 2017-09-30' ,'2017-10-01 2018-03-31' ,'2018-04-01 2018-09-30' ,'2018-10-01 2019-03-31' ,'2019-04-01 2019-09-30' ,'2019-10-01 2020-03-31' ,'2020-04-01 2020-09-30' ,'2020-10-01 2021-03-31' ,'2021-04-01 2021-09-30' ,'2021-10-01 2022-03-31' ,'2022-04-01 2022-09-30' ,'2022-10-01 2023-03-31' ,'2023-04-01 2023-09-30' ,'2023-10-01 2024-03-31']
trends_list = list()
for t in tf_list:
  pytrends.build_payload(kw_list, timeframe=t)
  trends_list.append(pytrends.interest_over_time())
combined_df = pd.concat(trends_list, axis=0)
combined_df.to_csv("data_src/Gtrends.csv")
```

# Google trends keywords

```{r}
# Gtrends has 7th column "is Partial" 
# which is not related.
Gtrends <- read.csv("data_src/Gtrends.csv")[-7]
knitr::kable(head(Gtrends))
```

# Merge the data

```{r}
merged<-merge(apple, Gtrends, 
	      by.x = "Date", by.y = "date", all=FALSE)
knitr::kable(head(merged))
```

```{r echo=FALSE}
write.csv(merged, "merged.csv")
```

# Scale the response variable

Plot the graphs of $y_t-y_{t-1}$

```{r echo=FALSE, fig.height=4}
par(mar = c(4, 5, 2, 4))
plot(difference(merged$AdjClosed)[-1], xlab="Date index", ylab=expression(paste(Y[t]-Y[t-1], sep="")), type="l", main=expression(paste("Changes in ", Y[t], " vs Date", sep="")))
```
```{r eval=FALSE, echo=FALSE}
pdf("pic/AdjClosed.pdf",height=3.5,width=7)
par(mar = c(4, 5, 2, 4))
plot(difference(merged$AdjClosed)[-1], xlab="Date index", ylab=expression(paste(Y[t]-Y[t-1], sep="")), type="l", main=expression(paste("Changes in ", Y[t], " vs Date", sep="")))
dev.off()
```

# Scale the response variable

Plot the graphs of $\sqrt{y_t}-\sqrt{y_{t-1}}$

```{r echo=FALSE, fig.height=4}
par(mar = c(4, 5, 2, 4))
plot(difference(sqrt(merged$AdjClosed))[-1], xlab="Date index", ylab=expression(paste(sqrt(Y[t])-sqrt(Y[t-1]), sep="")), type="l", main=expression(paste("Changes in ", sqrt(Y[t]), " vs Date", sep="")))
```
```{r eval=FALSE, echo=FALSE}
pdf("pic/Sqrt_AdjClosed.pdf",height=3.5,width=7)
par(mar = c(4, 5, 2, 4))
plot(difference(sqrt(merged$AdjClosed))[-1], xlab="Date index", ylab=expression(paste(sqrt(Y[t])-sqrt(Y[t-1]), sep="")), type="l", main=expression(paste("Changes in ", sqrt(Y[t]), " vs Date", sep="")))
dev.off()
```

# Scale the response variable

Plot the graphs of $\ln(y_t)-\ln(y_{t-1}) = \ln(\frac{y_t}{y_{t-1}})$

```{r echo=FALSE, fig.height=4}
par(mar = c(4, 5, 2, 4))
plot(difference(log(merged$AdjClosed))[-1], xlab="Date index", ylab=expression(paste(ln(Y[t])-ln(Y[t-1]), sep="")), type="l", main=expression(paste("Changes in ", ln(Y[t]), " vs Date", sep="")))
```
```{r eval=FALSE, echo=FALSE}
pdf("pic/log_AdjClosed.pdf",height=3.5,width=7)
par(mar = c(4, 5, 2, 4))
plot(difference(log(merged$AdjClosed))[-1], xlab="Date index", ylab=expression(paste(ln(Y[t])-ln(Y[t-1]), sep="")), type="l", main=expression(paste("Changes in ", ln(Y[t]), " vs Date", sep="")))
dev.off()
```

The log function appears to be more stable (almost constant variace over time) than the square root function, so we pick the log function to transform the response variable.

# Model definition: Autoregression with one variable

We use $y_{t}$ to represent the adjusted close price of Apple stock.
$y^{'}_{t} = \ln(y_{t})-\ln(y_{t-1}) = \ln(\frac{y_{t}}{y_{t-1}})$ represent the first difference of log of $y_{t}$.


Our AR($p$) model is defined as follows:

\[
y^{'}_{t} = c + \phi_{1}y^{'}_{t-1} + \phi_{2}y^{'}_{t-2} + \dots + \phi_{p}y^{'}_{t-p} + \varepsilon_{t},
\]

The predicted value for $y_t$ is calculated as follows:

\[
\hat{y}_t = \exp\{c + \phi_{1}y^{'}_{t-1} + \phi_{2}y^{'}_{t-2} + \dots + \phi_{p}y^{'}_{t-p}\}\cdot y_{t-1}
\]

# Model definition: Autoregression with multiple variables

From Google trends, we get the interest over time of keywords "Apple Watch", "MacBook", "AirPods", "iPad", "iPhone", represented by variable $y_{3,t},y_{4,t},y_{5,t},y_{6,t},y_{7,t}$ respectively. Also, we take the volume of the apple stock represented by variable $y_{2,t}$, the first difference of the adjusted close price of Apple stock as $y_{1,t}$ (which is also $y^{'}_t$). $\mathbf{y}_t = (y_{1,t},y_{2,t},y_{3,t},y_{4,t},y_{5,t},y_{6,t},y_{7,t})$.

We define our Vector Auto-Regressive(VAR) model as follows:

\[
y^{'}_t = \boldsymbol{\phi}_1\mathbf{y}_{t-1}^T+\ldots+\boldsymbol{\phi}_p\mathbf{y}_{t-p}^T+\epsilon_t
\]

where $\boldsymbol{\phi}_i$ are $(1\times7)$ coefficient vector for $i=1,\ldots,p$.


# Prepare the data: The feature vector Y

Here, variable `Y` represents $\mathbf{y}_t = (y_{1,t},y_{2,t},y_{3,t},y_{4,t},y_{5,t},y_{6,t},y_{7,t})$. It is a matrix and its first column is the same as the response variable $y^{'}_t$. The rest of `Y` is other features (Volumns and interests over time from Google trends)

```{r}
dy <- difference(log(merged$AdjClosed))[-1]
Y <- as.matrix(dy)
Y <- cbind(Y, merged$Volume[-1])
Y <- cbind(Y, merged$Apple.Watch[-1])
Y <- cbind(Y, merged$MacBook[-1])
Y <- cbind(Y, merged$AirPods[-1])
Y <- cbind(Y, merged$iPad[-1])
Y <- cbind(Y, merged$iPhone[-1])
```

# Prepare the data: Lagged features

Here we define our own function to prepare the lagged features. This function should return a dataframe with the first column as the response variable $y^{'}_t$. The rest columns of the output are the lagged features $\mathbf{y}_{t-1},\ldots,\mathbf{y}_{t-p}$

```{r}
prep_mx <- function(p=1, mat, y){
# prepare data for AR(p), 
# p is the number of lag steps.
# p is integer and p >= 1
  nr <- nrow(mat)
  result <- y[-c(1:p)]
  for (i in 1:p){
    result <- cbind(result, mat[c((p-i+1):((p-i+1+nr-p-1))),] )
  }
return(result)
}
```

# Check if the dataframe is in correct format

```{r}
mat <- as.matrix(dy); p<-3
UAR_Lag3 <- as.data.frame(prep_mx(p=p,mat,mat))
# New column names
new_names <- c("Yt")
for (i in 1:p){
	new_names <- c(new_names, sprintf("Yt_Lag%d",i))
}
# Rename columns using a loop
for (i in seq_along(new_names)) {
  colnames(UAR_Lag3)[i] <- new_names[i]
}
```

# Check if the dataframe is in correct format (Cont.)
 
```{r}
knitr::kable(head(UAR_Lag3))
dim(UAR_Lag3)
```

# Check the pairwise scatterplot of Lagged features

```{r fig.height = 5}
pairs(UAR_Lag3)
```
```{r echo=FALSE, eval=FALSE}
png("pic/pairwise.png")
pairs(UAR_Lag3)
dev.off()
```

This indicate that Lag 3 of the stock price itself does not provide enough infomation for prediction.


# Test run to check (I)

```{r}
lm(Yt~., data=UAR_Lag3)
```

# Test run to check (II)

```{r}
arima(mat, order=c(3,0,0))
```

# Autoregression on multivariate model

Because the `arima` function does not support regressions on multiple variables, so we use `lm` instead.

Also, if the $p$-Lag is large, we will have a large amount of predictor variables. Thus we need to do a variable selection. In this project, we do our variable selection using stepwise selection by AIC.

# Autoregression on multivariate model: Prepare the data

```{r}
mat <- as.matrix(dy)
MAR_Lag10 <- as.data.frame(prep_mx(p=10,Y,mat))
# New column names
p<-10
new_names <- c("Yt")
for (i in 1:p){
	new_names <- c(new_names, sprintf("Yt_Lag%d",i))
	new_names <- c(new_names, sprintf("Volume_Lag%d",i))
	new_names <- c(new_names, sprintf("Apple.Watch_Lag%d",i))
	new_names <- c(new_names, sprintf("MacBook_Lag%d",i))
	new_names <- c(new_names, sprintf("AirPods_Lag%d",i))
	new_names <- c(new_names, sprintf("iPad_Lag%d",i))
	new_names <- c(new_names, sprintf("iPhone_Lag%d",i))
}
# Rename columns using a loop
for (i in seq_along(new_names)) {
  colnames(MAR_Lag10)[i] <- new_names[i]
}
```

# Train Linear regression model usign 5-fold CV
We use first 1900 data as training and testing dataset, leave the last 100 data as testing dataset.
```{r eval=FALSE}
library(caret)

nrow(MAR_Lag10)
response <- MAR_Lag10[c(1:1900), 1]
predictors <- MAR_Lag10[c(1:1900), -1]

# Set up the control for cross-validation
control <- trainControl(method = "cv",
                        number = 5,
                        verboseIter = TRUE)
# Train the linear regression model using cross-validation
lm_model <- train(x=predictors, y=response, 
                  method = "lmStepAIC", 
                  trControl = control)
save(lm_model, file="lm_model.RData")
```

# Regression result

```{r echo=FALSE}
load("lm_model.RData")
```

```{r}
print(lm_model)
```

# Regression result (Cont)

```{r}
selected_variables <- rownames(as.matrix(
  lm_model$finalModel$coefficients))[-1]
selected_variables
```

# Plot the prediction on testing dataset

The black line is the true value of $y^{'}_t$; and the blue line is the predicted value of $y^{'}_t$

```{r fig.height=3.5}
par(mar = c(4, 5, 2, 4))
plot(x=c(1:100), MAR_Lag10[c(1901:2000),1], type="l",
 ylab=expression(Y[t]^"'"))
lines(x=c(1:100), as.matrix(cbind(rep(1, 100),
  MAR_Lag10[c(1901:2000),selected_variables]))%*%as.matrix(
  lm_model$finalModel$coefficients), col="blue")
```
```{r eval=FALSE, echo=FALSE}
pdf("pic/Predicted_dAdjColsed.pdf",height=3.5,width=7)
par(mar = c(4, 5, 2, 4))
plot(x=c(1:100), MAR_Lag10[c(1901:2000),1], type="l",
 ylab=expression(Y[t]^"'"),
 xlab="Data index",
 main=expression(paste("Predicted ",Y[t]^"'"," (in blue) and True value (in black)")))
lines(x=c(1:100), as.matrix(cbind(rep(1, 100),
  MAR_Lag10[c(1901:2000),selected_variables]))%*%as.matrix(
  lm_model$finalModel$coefficients), col="blue")
dev.off()
```

# Plot the prediction on testing dataset (Cont)

```{r fig.height=3.5}
# Lag-10 so true value starts from 1901+10=1911
par(mar = c(4, 5, 2, 4))
y_true= merged$AdjClosed[c(1911:2010)]
y_pred = exp(as.matrix(cbind(rep(1, 100),
  MAR_Lag10[c(1901:2000),selected_variables]))%*%as.matrix(
  lm_model$finalModel$coefficients))*
  merged$AdjClosed[c(1910:2009)] # Y_{t-1} starts from 1910
plot(y_true,type="l", ylab=expression(Y[t]))
lines(y_pred,type="l",col="blue")
```
```{r eval=FALSE, echo=FALSE}
pdf("pic/Predicted_AdjColsed.pdf",height=3.5,width=7)
# Lag-10 so true value starts from 1901+10=1911
par(mar = c(4, 5, 2, 4))
y_true= merged$AdjClosed[c(1911:2010)]
y_pred = exp(as.matrix(cbind(rep(1, 100),
  MAR_Lag10[c(1901:2000),selected_variables]))%*%as.matrix(
  lm_model$finalModel$coefficients))*
  merged$AdjClosed[c(1910:2009)] # Y_{t-1} starts from 1910
plot(y_true,type="l", ylab=expression(Y[t]), main=expression(paste("Predicted ",Y[t]," (in blue) and True value (in black)")))
lines(y_pred,type="l",col="blue")
dev.off()
```

